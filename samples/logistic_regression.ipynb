{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d68307920744071be9f01e3a82e9812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f3ae97f791b34b3fb469d6f02903a88d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d7878ddb9394406dac853e03488912b7",
              "IPY_MODEL_89a0c7724f0f4f53a70af0cf623a6911"
            ]
          }
        },
        "f3ae97f791b34b3fb469d6f02903a88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7878ddb9394406dac853e03488912b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_efc4d4fc5621463ea52bc1556937bf2a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef6f67e598ec4d9391ec5799fde9bba9"
          }
        },
        "89a0c7724f0f4f53a70af0cf623a6911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36c27d0a905649a78d178fb86085cd0d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 893kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2cf4084d28f74d0aa67f7dcfce33de3a"
          }
        },
        "efc4d4fc5621463ea52bc1556937bf2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef6f67e598ec4d9391ec5799fde9bba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36c27d0a905649a78d178fb86085cd0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2cf4084d28f74d0aa67f7dcfce33de3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9bd9b78266b408abe688dabdf840dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ecb29ee12bb34ada8ff2281ef63a1315",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0bcaf751f4354eb5952d1413abfd302f",
              "IPY_MODEL_6fa25af1c5394a74b5944aa1b4605fe0"
            ]
          }
        },
        "ecb29ee12bb34ada8ff2281ef63a1315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bcaf751f4354eb5952d1413abfd302f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d06429051924874b1b0f5d94624fab4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de76dcce192d46f980cafad4742a2c6a"
          }
        },
        "6fa25af1c5394a74b5944aa1b4605fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a58f1821ca894ffe954676c71ec7a3c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:06&lt;00:00, 64.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13730781022c4900a789cdd3db52d164"
          }
        },
        "9d06429051924874b1b0f5d94624fab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de76dcce192d46f980cafad4742a2c6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a58f1821ca894ffe954676c71ec7a3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13730781022c4900a789cdd3db52d164": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "15819e7ffcc647eba35283dd9ca812d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6421de2da9294721b776a5a783e6adb0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4c45638b743b4ef8a8cd5082f4f13ca7",
              "IPY_MODEL_61898a2ad287417eb32f4a6bf798bdf5"
            ]
          }
        },
        "6421de2da9294721b776a5a783e6adb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c45638b743b4ef8a8cd5082f4f13ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e8885d31803e4694a392cb48ed42ca83",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e250e3ae4dee49e48b707b9e98d80a00"
          }
        },
        "61898a2ad287417eb32f4a6bf798bdf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0164b70e760f413bbe2f5a1beed77545",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 67.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83e1c3ebf9c04b299b88dc1f663cab67"
          }
        },
        "e8885d31803e4694a392cb48ed42ca83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e250e3ae4dee49e48b707b9e98d80a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0164b70e760f413bbe2f5a1beed77545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83e1c3ebf9c04b299b88dc1f663cab67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FeOQWTMgh2A",
        "colab_type": "text"
      },
      "source": [
        "For the parts ```Bag of words``` and ```TF-IDF```, I was inspired by the [moocs  of coursera](https://www.coursera.org/learn/language-processing/home/week/1) that I followed recently.  \n",
        "For Bert, I was inspired by the reference [https://www.kaggle.com/rahulvks/distilbert-text-classification](https://www.kaggle.com/rahulvks/distilbert-text-classification) of the document [Technical Resources and Tutorials -- Pascal Notsawo summer 2020 Project 1](https://docs.google.com/document/d/1Sfev84E2mkF5rNNuvtZURlpYAhRw3NLmJqsJ2HQV--I/edit?usp=sharing) (shared drive folder), which itself is a replication of [https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEvvq1Za-YGd",
        "colab_type": "text"
      },
      "source": [
        "# **Data Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFaQRx5EhskV",
        "colab_type": "text"
      },
      "source": [
        "**Workspace**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuPZeB1Cgz71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm-MS143hM2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp -R /content/drive/\"My Drive\"/\"foo\"/Data /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH1U57g1hxOh",
        "colab_type": "text"
      },
      "source": [
        "**Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZxQV4kvhZZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "19ffaf8a-6f48-4da5-f12f-8eb8b0c068b6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pickle\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8C0hG8G0JJp",
        "colab_type": "text"
      },
      "source": [
        "**Import the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPnL4OL0h5Fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('Data/EULA_Training_Data_Set_1_v1.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RnAIz07yo97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91788616-35e2-4a63-ea71-a112124a79f7"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7879, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o94SC9SiNxi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4169158e-055d-48e5-f31a-d402f23d51cf"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clause ID</th>\n",
              "      <th>Clause Text</th>\n",
              "      <th>Classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1588</td>\n",
              "      <td>18. Governing Law: This Agreement shall be gov...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1146</td>\n",
              "      <td>1.8 Modification. We may modify, update, or di...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4792</td>\n",
              "      <td>Except as otherwise expressly provided in this...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2759</td>\n",
              "      <td>8.3.        The benefit and burdens of this Ag...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4400</td>\n",
              "      <td>DEFINITIONS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Clause ID                                        Clause Text  Classification\n",
              "0       1588  18. Governing Law: This Agreement shall be gov...               0\n",
              "1       1146  1.8 Modification. We may modify, update, or di...               1\n",
              "2       4792  Except as otherwise expressly provided in this...               0\n",
              "3       2759  8.3.        The benefit and burdens of this Ag...               1\n",
              "4       4400                                        DEFINITIONS               0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHiR8DmaKxPS",
        "colab_type": "text"
      },
      "source": [
        "**Summary of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ1TmBvn0Xu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0aqX7hy0n1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5a72badb-690b-4abb-e73a-4fe7452e1970"
      },
      "source": [
        "df['Classification'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    6407\n",
              "1    1472\n",
              "Name: Classification, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN7Fp396kyFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "training_pkl = pickle.load(open(\"Data/train.pkl\", \"rb\"))\n",
        "print(len(training_pkl))\n",
        "for x_y in training_pkl[:5]:\n",
        "  print(\"============\")\n",
        "  print(\"text : \" ,x_y[\"text\"])\n",
        "  print(\"target : \" ,x_y[\"target\"])\n",
        "print(\"============\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp_lxOCa1moi",
        "colab_type": "text"
      },
      "source": [
        "# **Spliting the training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjnuSjRLxdTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = df['Clause Text'].values, df['Classification'].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMsvQeU26XSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 1234\n",
        "test_ratio = 0.2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY0ccKUC5jU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_ratio, random_state = seed)\n",
        "#X_train, X_test, y_train, y_test = X[:6303], X[6304:], y[:6303], y[6304:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66f76VSUDh7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17a2e5c8-b3a9-4c45-a0a5-e1fd3ee5d860"
      },
      "source": [
        "len(X_train), len(X_test),"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6303, 1576)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvSA6tae9gDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "6d2ba2ac-979e-46c3-906a-9d970955b1c1"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Renewal.  Following the expiration of the Initial Term, this Agreement shall automatically renew for successive twelve (12) month periods (each, a “Renewal Term”), unless either party notifies the other party of its intent not to renew forty-five (45) or more days prior to the expiration of the Initial Term or any Renewal Term, as applicable.  The Initial Term and Renewal Terms are collectively referred to herein as the “Term.” '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIAWub7U7Usi",
        "colab_type": "text"
      },
      "source": [
        "**Text Prepare**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9mG5ahh7VKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def text_prepare(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lowercase text\n",
        "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = re.sub(BAD_SYMBOLS_RE, '', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwords from text\n",
        "    return text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyH8v-FJ7yb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = X_train[0]\n",
        "X_train = [text_prepare(x) for x in X_train]\n",
        "X_test = [text_prepare(x) for x in X_test]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ATJDGq74Rt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e9d25fbf-5808-4a4d-ac9b-920315b17a25"
      },
      "source": [
        "print(a)\n",
        "print(X_train[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Renewal.  Following the expiration of the Initial Term, this Agreement shall automatically renew for successive twelve (12) month periods (each, a “Renewal Term”), unless either party notifies the other party of its intent not to renew forty-five (45) or more days prior to the expiration of the Initial Term or any Renewal Term, as applicable.  The Initial Term and Renewal Terms are collectively referred to herein as the “Term.” \n",
            "renewal following expiration initial term agreement shall automatically renew successive twelve 12 month periods renewal term unless either party notifies party intent renew fortyfive 45 days prior expiration initial term renewal term applicable initial term renewal terms collectively referred herein term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKtrEzO49Iyj",
        "colab_type": "text"
      },
      "source": [
        "# **Transforming text to a vector**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_KXBgX29i6B",
        "colab_type": "text"
      },
      "source": [
        "## **1) Bag of words**   \n",
        "\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVN1PcNiAgSI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
        "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n",
        "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.  \n",
        "\n",
        "Let's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n",
        "\n",
        "    ['hi', 'you', 'me', 'are']\n",
        "\n",
        "Then we need to numerate them, for example, like this: \n",
        "\n",
        "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
        "\n",
        "And we have the text, which we want to transform to the vector:\n",
        "\n",
        "    'hi how are you'\n",
        "\n",
        "For this text we create a corresponding zero vector \n",
        "\n",
        "    [0, 0, 0, 0]\n",
        "    \n",
        "And iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n",
        "\n",
        "    'hi':  [1, 0, 0, 0]\n",
        "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
        "    'are': [1, 0, 0, 1]\n",
        "    'you': [1, 1, 0, 1]\n",
        "\n",
        "The resulting vector will be \n",
        "\n",
        "    [1, 1, 0, 1]\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQjwERoxAqiT",
        "colab_type": "text"
      },
      "source": [
        "To find the most common words use train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPJlp4WnLWTn",
        "colab_type": "text"
      },
      "source": [
        "**Words counts and most common words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvLzGKs6LaeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_counts = {}\n",
        "for line in X_train:\n",
        "  word_list = line.split()\n",
        "  for word in word_list: \n",
        "    words_counts[word] = words_counts.get(word, 0) + 1"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENNsEwfqLeNG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d86419a4-be32-4149-c385-f72572c6b328"
      },
      "source": [
        "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(most_common_words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('company', 8679), ('software', 5132), ('agreement', 4986), ('shall', 3898), ('use', 3578), ('services', 2907), ('customer', 2867), ('may', 2576), ('party', 2353), ('license', 2218)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoavvBFxD_7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DICT_SIZE = 10000 # size of the dictionary\n",
        "WORDS_TO_INDEX = {key: rank for rank, key in enumerate(sorted(words_counts.keys(), key=lambda x: words_counts[x], reverse=True)[:DICT_SIZE], 0)}\n",
        "INDEX_TO_WORDS = {y:x for x,y in WORDS_TO_INDEX.items()}\n",
        "ALL_WORDS = WORDS_TO_INDEX.keys()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twA0zTPVAPlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_bag_of_words(text, words_to_index, dict_size):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        dict_size: size of the dictionary\n",
        "        \n",
        "        return a vector which is a bag-of-words representation of 'text'\n",
        "    \"\"\"\n",
        "    result_vector = np.zeros(dict_size)\n",
        "    for item in text.split():\n",
        "        if item in words_to_index.keys():\n",
        "            result_vector[words_to_index[item]] += 1\n",
        "    return result_vector"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE9NNIBVBPLt",
        "colab_type": "text"
      },
      "source": [
        "Now apply the implemented function to all samples.  \n",
        "We use [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) (Compressed Sparse Row matrix) for fast matrix vector products and [scipy.sparse.vstack](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.vstack.html#scipy.sparse.vstack)  to Stack sparse matrices vertically (row wise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLpV_HCDBEcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sparse matrix package for numeric data.\n",
        "from scipy import sparse as sp_sparse "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSo252G9BHY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
        "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOa0j9DaoM5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79fed6e7-f7b2-432d-87c6-b8b1c693bc59"
      },
      "source": [
        "print('X_train shape ', X_train_mybag.shape)\n",
        "print('X_test shape ', X_test_mybag.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape  (6303, 10000)\n",
            "X_test shape  (1576, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxnxl1wvEgMp",
        "colab_type": "text"
      },
      "source": [
        "## 2) **TF-IDF**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWm8CHowFSgb",
        "colab_type": "text"
      },
      "source": [
        "TF-IDF takes into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \n",
        "\n",
        "- We use class [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from *scikit-learn*. \n",
        "- We use *train* corpus to train a vectorizer. \n",
        "- Our filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles)\n",
        "- We use bigrams along with unigrams in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DUGtA-qGfHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4cKTPq_G7R_",
        "colab_type": "text"
      },
      "source": [
        "How is it work?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bgGwTw_FRDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "b2c48ad3-9b8d-454c-d1d0-bbdc45a7bd83"
      },
      "source": [
        "corpus = [\n",
        "     'This is the first document.',\n",
        "     'This document is the second document.',\n",
        "     'And this is the third one.',\n",
        "     'Is this the first document?',\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_dummy = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.get_feature_names()) \n",
        "print(X_dummy.shape)\n",
        "print(X_dummy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "(4, 9)\n",
            "  (0, 1)\t0.46979138557992045\n",
            "  (0, 2)\t0.5802858236844359\n",
            "  (0, 6)\t0.38408524091481483\n",
            "  (0, 3)\t0.38408524091481483\n",
            "  (0, 8)\t0.38408524091481483\n",
            "  (1, 5)\t0.5386476208856763\n",
            "  (1, 1)\t0.6876235979836938\n",
            "  (1, 6)\t0.281088674033753\n",
            "  (1, 3)\t0.281088674033753\n",
            "  (1, 8)\t0.281088674033753\n",
            "  (2, 4)\t0.511848512707169\n",
            "  (2, 7)\t0.511848512707169\n",
            "  (2, 0)\t0.511848512707169\n",
            "  (2, 6)\t0.267103787642168\n",
            "  (2, 3)\t0.267103787642168\n",
            "  (2, 8)\t0.267103787642168\n",
            "  (3, 1)\t0.46979138557992045\n",
            "  (3, 2)\t0.5802858236844359\n",
            "  (3, 6)\t0.38408524091481483\n",
            "  (3, 3)\t0.38408524091481483\n",
            "  (3, 8)\t0.38408524091481483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHxlneWWHet2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_features(X_train, X_test):\n",
        "    \"\"\"\n",
        "        X_train, X_test — samples        \n",
        "        return TF-IDF vectorized representation of each sample and vocabulary\n",
        "    \"\"\"\n",
        "    # Create TF-IDF vectorizer with a proper parameters choice\n",
        "    # Fit the vectorizer on the train set\n",
        "    # Transform the train and test sets and return the result\n",
        "    \n",
        "    \n",
        "    tfidf_vectorizer = TfidfVectorizer(\n",
        "        lowercase = True, \n",
        "        min_df=5, \n",
        "        max_df=0.9, \n",
        "        ngram_range=(1, 2), \n",
        "        #token_pattern='(\\S+)' # todo\n",
        "    )\n",
        "    \n",
        "    X_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test = tfidf_vectorizer.transform(X_test)\n",
        "    \n",
        "    return X_train, X_test, tfidf_vectorizer, tfidf_vectorizer.vocabulary_"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVIJYqrjIF1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_tfidf, X_test_tfidf, tfidf_vectorizer, tfidf_vocab = tfidf_features(X_train, X_test)\n",
        "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOsvBNyOIxt5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2f14e50f-621a-4b4f-9c1c-6e5df42300b8"
      },
      "source": [
        "print('X_train_tfidf shape ', X_train_tfidf.shape)\n",
        "print('X_test_tfidf shape ', X_test_tfidf.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_tfidf shape  (6303, 11678)\n",
            "X_test_tfidf shape  (1576, 11678)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8-S3-r7IPgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4c1f1f47-e4ea-4f45-f3e7-8addf15ff5d2"
      },
      "source": [
        "assert list(tfidf_vocab.keys())[:10] == list(tfidf_reversed_vocab.values())[:10], \"An error occurred\"\n",
        "list(tfidf_vocab.keys())[:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['renewal',\n",
              " 'following',\n",
              " 'expiration',\n",
              " 'initial',\n",
              " 'term',\n",
              " 'agreement',\n",
              " 'shall',\n",
              " 'automatically',\n",
              " 'renew',\n",
              " 'successive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk3BVtcBTwyj",
        "colab_type": "text"
      },
      "source": [
        "## 3) **BERT**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ-0B3pqPj0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "1c29a9f7-78e9-410d-c661-0bfeca9ad100"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 5.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 30.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 20.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3be294c7d4cb6216a0303d02de85511cc9f7dfaf463d05b544d8d0383e91ed55\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QFCqOZ9hCb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import itertools\n",
        "import torch\n",
        "import transformers as tfm\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAGW3vxpPqQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnLXMWA7hMl_",
        "colab_type": "text"
      },
      "source": [
        "**Loading the Pre-trained BERT model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Aljg0nOSNVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "7d68307920744071be9f01e3a82e9812",
            "f3ae97f791b34b3fb469d6f02903a88d",
            "d7878ddb9394406dac853e03488912b7",
            "89a0c7724f0f4f53a70af0cf623a6911",
            "efc4d4fc5621463ea52bc1556937bf2a",
            "ef6f67e598ec4d9391ec5799fde9bba9",
            "36c27d0a905649a78d178fb86085cd0d",
            "2cf4084d28f74d0aa67f7dcfce33de3a",
            "c9bd9b78266b408abe688dabdf840dea",
            "ecb29ee12bb34ada8ff2281ef63a1315",
            "0bcaf751f4354eb5952d1413abfd302f",
            "6fa25af1c5394a74b5944aa1b4605fe0",
            "9d06429051924874b1b0f5d94624fab4",
            "de76dcce192d46f980cafad4742a2c6a",
            "a58f1821ca894ffe954676c71ec7a3c2",
            "13730781022c4900a789cdd3db52d164",
            "15819e7ffcc647eba35283dd9ca812d6",
            "6421de2da9294721b776a5a783e6adb0",
            "4c45638b743b4ef8a8cd5082f4f13ca7",
            "61898a2ad287417eb32f4a6bf798bdf5",
            "e8885d31803e4694a392cb48ed42ca83",
            "e250e3ae4dee49e48b707b9e98d80a00",
            "0164b70e760f413bbe2f5a1beed77545",
            "83e1c3ebf9c04b299b88dc1f663cab67"
          ]
        },
        "outputId": "b57660e3-5d86-49b5-d748-a0160b0afef4"
      },
      "source": [
        "## For BERT :\n",
        "model_class, tokenizer_class, pretrained_weights = (tfm.BertModel, tfm.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "model = model_class.from_pretrained(pretrained_weights)\n",
        "model = model.to(device)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d68307920744071be9f01e3a82e9812",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9bd9b78266b408abe688dabdf840dea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15819e7ffcc647eba35283dd9ca812d6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8It1tbGRRfup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_and_batching(df, pad_len, batch_size = 32, n_samples = None):\n",
        "    data = []\n",
        "    i = 0 \n",
        "    if n_samples :\n",
        "      n_samples = min(n_samples, df.shape[0])\n",
        "    else :\n",
        "      n_samples = df.shape[0]\n",
        "\n",
        "    ### Tokenization\n",
        "    a = [len(df['Clause Text'][i]) for i in range(100)]\n",
        "    print(a)\n",
        "    tokenized = df['Clause Text'].apply(lambda x : tokenizer.encode(x, add_special_tokens=True))\n",
        "    \n",
        "    ## Padding\n",
        "    padded = pad_sequences(sequences = tokenized, maxlen= pad_len, dtype = 'int64', truncating=\"post\", padding=\"post\")\n",
        "      \n",
        "    ### Batching\n",
        "    input_ids = []\n",
        "    while n_samples > i :\n",
        "        i += batch_size\n",
        "        input_ids.append(torch.LongTensor(padded[i-batch_size:i]).to(device))  \n",
        "    \n",
        "    ## Masking\n",
        "    attention_mask = [np.where(batch.cpu().numpy() != 0, 1, 0) for batch in input_ids]  \n",
        "    attention_mask = [torch.LongTensor(batch).to(device) for batch in attention_mask]\n",
        "    \n",
        "    return input_ids, attention_mask, df['Classification'][:i]\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LDYj6zFhsBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids, attention_mask, labels = pad_and_batching(\n",
        "    df = df, \n",
        "    pad_len = max_input_length, # equal to model.config.to_dict()['max_position_embeddings']\n",
        "    batch_size = 32,\n",
        "    n_samples = None # use all the data\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Easg_hXhxVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "02811f4b-ad0a-4733-9d94-5eed80ac7ea2"
      },
      "source": [
        "print(len(input_ids), input_ids[0].shape)\n",
        "print(len(attention_mask), attention_mask[0].shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "247 torch.Size([32, 512])\n",
            "247 torch.Size([32, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJl_NtN-iiCD",
        "colab_type": "text"
      },
      "source": [
        "Model - The model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFccPG-JikUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_hidden_states = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for input_ids_batch, attention_mask_batch in zip(input_ids, attention_mask) :\n",
        "        last_hidden_states.append(\n",
        "            model(input_ids_batch, attention_mask = attention_mask_batch)\n",
        "        )      "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCKwAf35yCXa",
        "colab_type": "text"
      },
      "source": [
        "**Free CUDA memory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oqLiz0Ix3fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = [batch.cpu() for batch in input_ids]\n",
        "attention_mask = [batch.cpu() for batch in attention_mask]\n",
        "model = model.cpu()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZkvK-gAiTMr",
        "colab_type": "text"
      },
      "source": [
        "We'll save those in the features variable, as they'll serve as the features to our logitics regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBUkfK-WiHIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten\n",
        "features = list(\n",
        "    itertools.chain.from_iterable(\n",
        "      [batch[0][:,0,:].cpu().numpy() for batch in last_hidden_states]\n",
        "    )\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWF6mZVjieDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(features, labels, test_size = test_ratio, random_state = seed)\n",
        "#X_train_bert, X_test_bert, y_train_bert, y_test_bert = features[:6303], features[6304:], labels[:6303], labels[6304:]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YigFHk3nrw8s",
        "colab_type": "text"
      },
      "source": [
        "### **Same process for distilbert**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xsGu9EJrwaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For DistilBERT:\n",
        "model_class2, tokenizer_class2, pretrained_weights2 = (tfm.DistilBertModel, tfm.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "tokenizer2 = tokenizer_class2.from_pretrained(pretrained_weights2)\n",
        "max_input_length2 = tokenizer2.max_model_input_sizes['distilbert-base-uncased']\n",
        "model2 = model_class2.from_pretrained(pretrained_weights2)\n",
        "model2 = model2.to(device)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFTgvdUhruMc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f2cb508-da53-46ff-a6b9-2aeffb1fa421"
      },
      "source": [
        "input_ids2, attention_mask2, labels2 = pad_and_batching(\n",
        "    df = df, \n",
        "    pad_len = max_input_length2, # equal to model2.config.to_dict()['max_position_embeddings']\n",
        "    batch_size = 32,\n",
        "    n_samples = None # use all the data\n",
        ")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (749 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[748, 482, 722, 147, 11, 594, 638, 253, 502, 138, 726, 30, 878, 56, 528, 15, 192, 303, 69, 227, 382, 248, 993, 251, 227, 482, 1702, 7, 599, 1416, 190, 484, 518, 30, 221, 232, 2358, 28, 2635, 55, 383, 534, 872, 16, 1804, 265, 1068, 1078, 1424, 639, 236, 697, 462, 177, 421, 1498, 47, 15, 122, 125, 589, 16, 241, 201, 412, 1794, 431, 316, 168, 356, 961, 421, 54, 12, 9, 855, 253, 83, 12, 764, 120, 10, 21, 370, 609, 1761, 3662, 55, 19, 122, 1448, 584, 101, 535, 319, 218, 222, 293, 313, 257]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (790 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (987 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1177 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (936 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1008 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (768 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1169 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (788 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (716 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (895 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1017 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (976 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (679 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1673 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (782 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1037 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (808 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1213 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (9201 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3645 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2352 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (885 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akxK01tcscib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_hidden_states2 = []\n",
        "model2.eval()\n",
        "with torch.no_grad():\n",
        "    for input_ids_batch, attention_mask_batch in zip(input_ids2, attention_mask2) :\n",
        "        last_hidden_states2.append(\n",
        "            model2(input_ids_batch, attention_mask = attention_mask_batch)\n",
        "        )"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVEX9sv4y0WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids2 = [batch.cpu() for batch in input_ids2]\n",
        "attention_mask2 = [batch.cpu() for batch in attention_mask2]\n",
        "model2 = model2.cpu()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqNmgQ-etbM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features2 = list(\n",
        "    itertools.chain.from_iterable(\n",
        "      [batch[0][:,0,:].cpu().numpy() for batch in last_hidden_states2]\n",
        "    )\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gQ4CwrItg16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_distilbert, X_test_distilbert, y_train_distilbert, y_test_distilbert = train_test_split(features2, labels2, test_size = test_ratio, random_state = seed)\n",
        "#X_train_distilbert, X_test_distilbert, y_train_distilbert, y_test_distilbert = features2[:6303], features2[6304:], labels2[:6303], labels2[6304:]"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdgYGVKWJqCU",
        "colab_type": "text"
      },
      "source": [
        "# **Classifiers**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQDj9tqXQ4ek",
        "colab_type": "text"
      },
      "source": [
        "## **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_hPQU5OKORN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV  "
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGOiKveG6AC",
        "colab_type": "text"
      },
      "source": [
        "### **Exhaustive search over specified parameter values for an estimator.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q4_V7i6IEr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'C': np.linspace(start = 0.0001, stop= 100, num=100)}"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7DpU0QCIW2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search_mybag = GridSearchCV(LogisticRegression(), parameters, n_jobs = -1)\n",
        "grid_search_tfidf = GridSearchCV(LogisticRegression(), parameters, n_jobs = -1)\n",
        "grid_search_bert = GridSearchCV(LogisticRegression(), parameters, n_jobs = -1)\n",
        "grid_search_distilbert = GridSearchCV(LogisticRegression(), parameters, n_jobs = -1)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxqgUwtGIpzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search_mybag.fit(X_train_mybag, y_train)\n",
        "grid_search_tfidf.fit(X_train_tfidf, y_train)\n",
        "grid_search_bert.fit(X_train_bert, y_train_bert)\n",
        "grid_search_distilbert.fit(X_train_distilbert, y_train_distilbert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7kWiT2-GkrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('best parameters mybag: ', grid_search_mybag.best_params_)\n",
        "print('best scrores mybag: ', grid_search_mybag.best_score_)\n",
        "\n",
        "print('best parameters tfidf: ', grid_search_tfidf.best_params_)\n",
        "print('best scrores tfidf: ', grid_search_tfidf.best_score_)\n",
        "\n",
        "print('best parameters bert: ', grid_search_bert.best_params_)\n",
        "print('best scrores bert: ', grid_search_bert.best_score_)\n",
        "\n",
        "print('best parameters distilbert: ', grid_search_distilbert.best_params_)\n",
        "print('best scrores distilbert: ', grid_search_distilbert.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDF3yFhuaMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C_mybag = \n",
        "C_tfidf = \n",
        "C_bert = \n",
        "C_distilbert = "
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZHMps8hOE9a",
        "colab_type": "text"
      },
      "source": [
        "Train the classifiers for different data transformations: *bag-of-words*, *tf-idf* and *bert*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUFM8QhmKxiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier_mybag = LogisticRegression(penalty = \"l2\", C = C_mybag, solver = \"newton-cg\", random_state = 0, n_jobs = -1).fit(\n",
        "    X_train_mybag, y_train\n",
        ")\n",
        "\n",
        "classifier_tfidf = LogisticRegression(penalty = \"l2\", C = C_tfidf, solver = \"newton-cg\", random_state = 0, n_jobs = -1).fit(\n",
        "    X_train_tfidf, \n",
        "    y_train\n",
        ")\n",
        "\n",
        "classifier_bert = LogisticRegression(penalty = \"l2\", C = C_bert, solver = \"newton-cg\", random_state = 0, n_jobs = -1).fit(\n",
        "    X_train_bert, \n",
        "    y_train_bert\n",
        ")\n",
        "\n",
        "classifier_distilbert = LogisticRegression(penalty = \"l2\", C = C_distilbert, solver = \"newton-cg\", random_state = 0, n_jobs = -1).fit(\n",
        "    X_train_distilbert, \n",
        "    y_train_distilbert\n",
        ")"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itetoF3cOJDF",
        "colab_type": "text"
      },
      "source": [
        "Create predictions for the data : labels and scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdgM_f_nK12P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
        "y_test_predicted_scores_mybag = classifier_mybag.decision_function(X_test_mybag)\n",
        "\n",
        "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
        "y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(X_test_tfidf)\n",
        "\n",
        "y_test_predicted_labels_bert = classifier_bert.predict(X_test_bert)\n",
        "y_test_predicted_scores_bert = classifier_bert.decision_function(X_test_bert)\n",
        "\n",
        "y_test_predicted_labels_distilbert = classifier_distilbert.predict(X_test_distilbert)\n",
        "y_test_predicted_scores_distilbert = classifier_distilbert.decision_function(X_test_distilbert)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCzbiZkdvjgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('===== Bag-of-words : ', classifier_mybag.score(X_test_mybag, y_test))\n",
        "print('===== Tfidf : ', classifier_tfidf.score(X_test_tfidf, y_test))\n",
        "print('===== Bert : ', classifier_bert.score(X_test_bert, y_test_bert))\n",
        "print('===== distilBert : ', classifier_distilbert.score(X_test_distilbert, y_test_distilbert))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMuZaV1xOgj0",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "To evaluate the results we will use several classification metrics:\n",
        " - [Accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
        " - [F1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOe1RvfIMhnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score  "
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSOLUDebPAk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_evaluation_scores(y, predicted):\n",
        "    print(\"accuracy_score : \", accuracy_score(y, predicted))\n",
        "    print(\"f1_score : \", f1_score(y, predicted, average=\"macro\"))\n",
        "    print(\"recall_score : \", recall_score(y, predicted, average=\"macro\"))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVpbDzSFPEcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('===== Bag-of-words')\n",
        "print_evaluation_scores(y_test, y_test_predicted_labels_mybag)\n",
        "print('===== Tfidf')\n",
        "print_evaluation_scores(y_test, y_test_predicted_labels_tfidf)\n",
        "print('===== Bert')\n",
        "print_evaluation_scores(y_test_bert, y_test_predicted_labels_bert)\n",
        "print('===== distilBert')\n",
        "print_evaluation_scores(y_test_distilbert, y_test_predicted_labels_distilbert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErxPGsH1Hm91",
        "colab_type": "text"
      },
      "source": [
        "### **Save for production** [production.pth](https://github.com/Tikquuss/eulascript)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkdBAmViIBx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE8gcfnvH2FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "production = {\n",
        "   # Bag of word\n",
        "   \"words_counts\" : words_counts,\n",
        "   \"WORDS_TO_INDEX\" : WORDS_TO_INDEX,\n",
        "   \"DICT_SIZE\" : DICT_SIZE,\n",
        "   \"classifier_mybag\" : classifier_mybag,\n",
        "\n",
        "   # TF-IDF \n",
        "   \"tfidf_vectorizer\" : tfidf_vectorizer,\n",
        "   \"classifier_tfidf\" : classifier_tfidf,\n",
        "\n",
        "   # Bert & distilBert\n",
        "    \"classifier_bert\" : classifier_bert,\n",
        "    \"classifier_distilbert\" : classifier_distilbert,    \n",
        "}"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFGCO-R_IDUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(production, open('/content/production.pth', 'wb'))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5nDMbNNKsoj",
        "colab_type": "text"
      },
      "source": [
        "### **Deploy model with gradio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1d9utOjK9eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install gradio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxcy7apPLVWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gradio as gr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH9tSnqTLXv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mybag_predict(eula):\n",
        "    vec = my_bag_of_words(text_prepare(eula) , WORDS_TO_INDEX, DICT_SIZE)\n",
        "    output = classifier_mybag.predict([vec])[0]\n",
        "    return \"EULA acceptable\" if output == 1 else \"EULA unacceptable\"\n",
        "\n",
        "def tfidf_predict(eula):\n",
        "    vec = tfidf_vectorizer.transform([text_prepare(eula)])\n",
        "    output = classifier_tfidf.predict(vec)[0]\n",
        "    return \"EULA acceptable\" if output == 1 else \"EULA unacceptable\"\n",
        "\n",
        "def bert_predict(eula):\n",
        "  tokens = tokenizer.tokenize(eula)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  init_token_idx = tokenizer.cls_token_id\n",
        "  eos_token_idx = tokenizer.sep_token_id\n",
        "  indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "        pooled_output, _ = model(tensor)\n",
        "  vec = pooled_output[:,0,:].cpu().numpy()\n",
        "  output = classifier_bert.predict(vec)[0]\n",
        "  return \"EULA acceptable\" if output == 1 else \"EULA unacceptable\"\n",
        "\n",
        "def predict(model_name, eula):\n",
        "  if model_name == \"Bag of word\":\n",
        "    return mybag_predict(eula)\n",
        "  elif model_name == \"TD-IDF\":\n",
        "    return tfidf_predict(eula)\n",
        "  elif model_name == \"BERT\":\n",
        "    return bert_predict(eula)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijgkud5PikJx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_TRVcECLf39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = gr.inputs.Textbox(placeholder=\"Your end-user license agreements\", label = \"EULA\", lines=20)\n",
        "output = gr.outputs.Textbox()\n",
        "gr.Interface(fn = mybag_predict, inputs = inputs, outputs = output).launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QluOp4_Fiy3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = gr.inputs.Textbox(placeholder=\"Your end-user license agreements\", label = \"EULA\", lines=20)\n",
        "output = gr.outputs.Textbox()\n",
        "gr.Interface(fn = tfidf_predict, inputs = inputs, outputs = output).launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r1WkANt2IFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = gr.inputs.Textbox(placeholder=\"Your end-user license agreements\", label = \"EULA\", lines=20)\n",
        "output = gr.outputs.Textbox()\n",
        "gr.Interface(fn = bert_predict, inputs = inputs, outputs = output).launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjkHV3rp4LrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = gr.inputs.Textbox(placeholder=\"Your end-user license agreements\", label = \"EULA\", lines=20)\n",
        "output = gr.outputs.Textbox()\n",
        "gr.Interface(fn = bert_predict, inputs = inputs, outputs = output).launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzIEBU2K6_pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = gr.inputs.Textbox(placeholder=\"Your end-user license agreements\", label = \"EULA\", lines=20)\n",
        "model_name = gr.inputs.Dropdown([\"Bag of word\", \"TD-IDF\", \"BERT\"], label = \"model name\")\n",
        "output = gr.outputs.Textbox()\n",
        "gr.Interface(fn = predict, inputs = [model_name, inputs], outputs = output).launch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p4IAhOd_wdX",
        "colab_type": "text"
      },
      "source": [
        "## **Gaussian Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svfDbQHL_6QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb = gnb.fit(X_train_bert, y_train_bert)\n",
        "\n",
        "gnb.score(X_test_bert,  y_test_bert)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}